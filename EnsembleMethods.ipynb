{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marukos/Ensemble-Methods/blob/main/EnsembleMethods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8gU7AYPXMmA"
      },
      "source": [
        "## About iPython Notebooks ##\n",
        "\n",
        "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. Make sure you fill in any place that says `# BEGIN CODE HERE #END CODE HERE`. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run\" (denoted by a play symbol). Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
        "\n",
        " **What you need to remember:**\n",
        "\n",
        "- Run your cells using SHIFT+ENTER (or \"Run cell\")\n",
        "- Write code in the designated areas using Python 3 only\n",
        "- Do not modify the code outside of the designated areas\n",
        "- In some cases you will also need to explain the results. There will also be designated areas for that.\n",
        "\n",
        "Fill in your **NAME** and **AEM** below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO-jJrtNXMmH"
      },
      "outputs": [],
      "source": [
        "NAME = \"Markos Koletsas\"\n",
        "AEM = \"3557\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh0EE7BJXMmJ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_VpnGyWXMmK"
      },
      "source": [
        "# Assignment 3 - Ensemble Methods #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dQ9XoGQXMmK"
      },
      "source": [
        "Welcome to your third assignment. This exercise will test your understanding on Ensemble Methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvHYIhS-XMmL"
      },
      "outputs": [],
      "source": [
        "# Always run this cell\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, make_scorer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier,  ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, cross_validate\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# USE THE FOLLOWING RANDOM STATE FOR YOUR CODE\n",
        "RANDOM_STATE = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joKwpih2XMmM"
      },
      "source": [
        "## Download the Dataset ##\n",
        "Download the dataset using the following cell or from this [link](https://github.com/sakrifor/public/tree/master/machine_learning_course/EnsembleDataset) and put the files in the same folder as the .ipynb file.\n",
        "In this assignment you are going to work with a dataset originated from the [ImageCLEFmed: The Medical Task 2016](https://www.imageclef.org/2016/medical) and the **Compound figure detection** subtask. The goal of this subtask is to identify whether a figure is a compound figure (one image consists of more than one figure) or not. The train dataset consits of 4197 examples/figures and each figure has 4096 features which were extracted using a deep neural network. The *CLASS* column represents the class of each example where 1 is a compoung figure and 0 is not.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJdwPr0bXMmM",
        "outputId": "4c8acdb9-f246-4853-c8f0-0bb5216e9bb2",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('test_set_noclass.csv', <http.client.HTTPMessage at 0x228f9343910>)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import urllib.request\n",
        "url_train = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/train_set.csv'\n",
        "filename_train = 'train_set.csv'\n",
        "urllib.request.urlretrieve(url_train, filename_train)\n",
        "url_test = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/test_set_noclass.csv'\n",
        "filename_test = 'test_set_noclass.csv'\n",
        "urllib.request.urlretrieve(url_test, filename_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0OVtYr7XMmN"
      },
      "outputs": [],
      "source": [
        "# Run this cell to load the data\n",
        "train_set = pd.read_csv(\"train_set.csv\").sample(frac=1).reset_index(drop=True)\n",
        "train_set.head()\n",
        "X = train_set.drop(columns=['CLASS'])\n",
        "y = train_set['CLASS'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxOGHSmqXMmO"
      },
      "source": [
        "## 1.0 Testing different ensemble methods ##\n",
        "In this part of the assignment you are asked to create and test different ensemble methods using the train_set.csv dataset. You should use **10-fold cross validation** for your tests and report the average f-measure weighted and balanced accuracy of your models. You can use [cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) and select both metrics to be measured during the evaluation. Otherwise, you can use [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold).\n",
        "\n",
        "### !!! Use n_jobs=-1 where is posibble to use all the cores of a machine for running your tests ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww_u4OlrXMmO"
      },
      "source": [
        "### 1.1 Voting ###\n",
        "Create a voting classifier which uses three **simple** estimators/classifiers. Test both soft and hard voting and choose the best one. Consider as simple estimators the following:\n",
        "\n",
        "\n",
        "*   Decision Trees\n",
        "*   Linear Models\n",
        "*   Probabilistic Models (Naive Bayes)\n",
        "*   KNN Models  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwvPacgkXMmP"
      },
      "outputs": [],
      "source": [
        "# BEGIN CODE HERE\n",
        "\n",
        "cls1 = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=1) # Classifier #1\n",
        "cls2 = KNeighborsClassifier(n_jobs=-1, n_neighbors=7) # Classifier #2\n",
        "cls3 = LogisticRegression(n_jobs=-1, random_state=RANDOM_STATE) # Classifier #3\n",
        "\n",
        "soft_vcls = VotingClassifier(voting='soft', estimators=[('Decision Tree', cls1), ('7NN', cls2),\n",
        "                                                        ('Logistic Regression', cls3)],\n",
        "                             n_jobs=-1) # Voting Classifier\n",
        "\n",
        "hard_vcls = VotingClassifier(voting='hard', estimators=[('Decision Tree', cls1), ('KNN', cls2),\n",
        "                                                        ('Logistic Regression', cls3)],\n",
        "                             n_jobs=-1) # Voting Classifier\n",
        "\n",
        "svlcs_scores = cross_validate(soft_vcls, X, y, n_jobs=-1, scoring={'f1 weighted':make_scorer(f1_score, average='weighted'),\n",
        "                                                                   'balanced accuracy':make_scorer(balanced_accuracy_score)},\n",
        "                              cv=10)\n",
        "\n",
        "s_avg_fmeasure = sum(svlcs_scores['test_f1 weighted'])/len(svlcs_scores['test_f1 weighted']) # The average f-measure\n",
        "s_avg_accuracy = sum(svlcs_scores['test_balanced accuracy'])/len(svlcs_scores['test_balanced accuracy']) # The average accuracy\n",
        "\n",
        "hvlcs_scores = cross_validate(hard_vcls, X, y, n_jobs=-1, scoring={'f1 weighted':make_scorer(f1_score, average='weighted'),\n",
        "                                                                   'balanced accuracy':make_scorer(balanced_accuracy_score)},\n",
        "                              cv=10)\n",
        "\n",
        "h_avg_fmeasure = sum(hvlcs_scores['test_f1 weighted'])/len(hvlcs_scores['test_f1 weighted']) # The average f-measure\n",
        "h_avg_accuracy = sum(hvlcs_scores['test_balanced accuracy'])/len(hvlcs_scores['test_balanced accuracy']) # The average accuracy\n",
        "#END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQQvClrmXMmQ",
        "outputId": "00c56f60-1360-42e6-8c19-dd602c54ac16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier:\n",
            "VotingClassifier(estimators=[('Decision Tree',\n",
            "                              DecisionTreeClassifier(max_depth=1,\n",
            "                                                     random_state=42)),\n",
            "                             ('7NN',\n",
            "                              KNeighborsClassifier(n_jobs=-1, n_neighbors=7)),\n",
            "                             ('Logistic Regression',\n",
            "                              LogisticRegression(n_jobs=-1, random_state=42))],\n",
            "                 n_jobs=-1, voting='soft')\n",
            "F1 Weighted-Score: 0.8413 & Balanced Accuracy: 0.8334\n"
          ]
        }
      ],
      "source": [
        "print(\"Classifier:\")\n",
        "print(soft_vcls)\n",
        "print(\"F1 Weighted-Score: {} & Balanced Accuracy: {}\".format(round(s_avg_fmeasure,4), round(s_avg_accuracy,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-iJK9pFaDka"
      },
      "source": [
        "You should achive above 82% (Soft Voting Classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRNkVAvEYVbn",
        "outputId": "f6ac8c47-fc2c-48bc-806d-9d16c6964a79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier:\n",
            "VotingClassifier(estimators=[('Decision Tree',\n",
            "                              DecisionTreeClassifier(max_depth=1,\n",
            "                                                     random_state=42)),\n",
            "                             ('KNN',\n",
            "                              KNeighborsClassifier(n_jobs=-1, n_neighbors=7)),\n",
            "                             ('Logistic Regression',\n",
            "                              LogisticRegression(n_jobs=-1, random_state=42))],\n",
            "                 n_jobs=-1)\n",
            "F1 Weighted-Score: 0.8215 & Balanced Accuracy: 0.8084\n"
          ]
        }
      ],
      "source": [
        "print(\"Classifier:\")\n",
        "print(hard_vcls)\n",
        "print(\"F1 Weighted-Score: {} & Balanced Accuracy: {}\".format(round(h_avg_fmeasure,4), round(h_avg_accuracy,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6M0CZO6aEHi"
      },
      "source": [
        "You should achieve above 80% in both! (Hard Voting Classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVPuIxwFXMmR"
      },
      "source": [
        "### 1.2 Stacking ###\n",
        "Create a stacking classifier which uses two more complex estimators. Try different simple classifiers (like the ones mentioned before) for the combination of the initial estimators. Report your results in the following cell.\n",
        "\n",
        "Consider as complex estimators the following:\n",
        "\n",
        "*   Random Forest\n",
        "*   SVM\n",
        "*   Gradient Boosting\n",
        "*   MLP\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX6T1qrFXMmS"
      },
      "outputs": [],
      "source": [
        "# BEGIN CODE HERE\n",
        "\n",
        "cls1 = MLPClassifier(random_state=RANDOM_STATE) # Classifier #1\n",
        "cls2 = LinearSVC(random_state=RANDOM_STATE, max_iter=500000) # Classifier #2\n",
        "cls3 = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "cls4 = '' # Classifier #3 (Optional)(Final Estimator)\n",
        "scls = StackingClassifier(estimators=[('MLP',cls1),('Linear SVM',cls2),('Gradient Boosting', cls3)], n_jobs=-1) # Stacking Classifier\n",
        "\n",
        "scores = cross_validate(scls, X, y, n_jobs=-1, scoring={'f1 weighted':make_scorer(f1_score, average='weighted'),\n",
        "                                                        'balanced accuracy':make_scorer(balanced_accuracy_score)}, cv=10)\n",
        "\n",
        "avg_fmeasure = sum(scores['test_f1 weighted'])/len(scores['test_f1 weighted']) # The average f-measure\n",
        "avg_accuracy = sum(scores['test_balanced accuracy'])/len(scores['test_balanced accuracy']) # The average accuracy\n",
        "#END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JLRzkQ1XMmT",
        "outputId": "56e8cde1-51b2-4d5a-d84e-02a8a17ff099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier:\n",
            "StackingClassifier(estimators=[('MLP', MLPClassifier(random_state=42)),\n",
            "                               ('Linear SVM',\n",
            "                                LinearSVC(max_iter=500000, random_state=42)),\n",
            "                               ('Gradient Boosting',\n",
            "                                GradientBoostingClassifier(random_state=42))],\n",
            "                   n_jobs=-1)\n",
            "F1 Weighted Score: 0.8556 & Balanced Accuracy: 0.8489\n"
          ]
        }
      ],
      "source": [
        "print(\"Classifier:\")\n",
        "print(scls)\n",
        "print(\"F1 Weighted Score: {} & Balanced Accuracy: {}\".format(round(avg_fmeasure,4), round(avg_accuracy,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcgOx-HPvBI-"
      },
      "source": [
        "You should achieve above 85% in both"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-nqW51xXMmU"
      },
      "source": [
        "## 2.0 Randomization ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPG8MdFLXMmV"
      },
      "source": [
        "**2.1** You are asked to create three ensembles of decision trees where each one uses a different method for producing homogeneous ensembles. Compare them with a simple decision tree classifier and report your results in the dictionaries (dict) below using as key the given name of your classifier and as value the f1_weighted/balanced_accuracy score. The dictionaries should contain four different elements.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmkaP-DjXMmV"
      },
      "outputs": [],
      "source": [
        "# BEGIN CODE HERE\n",
        "\n",
        "forest=[\n",
        "    DecisionTreeClassifier(random_state=RANDOM_STATE, max_features='auto'),\n",
        "    DecisionTreeClassifier(random_state=RANDOM_STATE, max_features='sqrt'),\n",
        "    DecisionTreeClassifier(random_state=RANDOM_STATE, max_features='log2')\n",
        "]\n",
        "\n",
        "voting_soft = VotingClassifier(voting='soft', estimators=[('Criterion Entropy', forest[0]), ('Criterion Log Loss', forest[1]),\n",
        "                                                        ('Criterion Gini', forest[2])],\n",
        "                             n_jobs=-1) # Voting Classifier\n",
        "\n",
        "voting_hard = voting_soft = VotingClassifier(voting='hard', estimators=[('Criterion Entropy', forest[0]), ('Criterion Log Loss', forest[1]),\n",
        "                                                                        ('Criterion Gini', forest[2])],\n",
        "                             n_jobs=-1) # Voting Classifier\n",
        "\n",
        "stack = StackingClassifier(estimators=[('Criterion Entropy', forest[0]), ('Criterion Log Loss', forest[1]),\n",
        "                                       ('Criterion Gini', forest[2])],\n",
        "                           n_jobs=-1) # Stacking Classifier\n",
        "\n",
        "\n",
        "ens1 = BaggingClassifier(base_estimator=voting_soft, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "ens2 = BaggingClassifier(base_estimator=voting_hard, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "ens3 = BaggingClassifier(base_estimator=stack, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "tree = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
        "\n",
        "f_measures = dict()\n",
        "accuracies = dict()\n",
        "\n",
        "for classifier, name in [(ens1,'Bugging and Soft Voting Trees'), (ens2,'Bugging with Hard Voting Trees'),\n",
        "                        (ens3,'Bugging Stacking Trees'), (tree, 'Simple Decision')]:\n",
        "\n",
        "    scores = cross_validate(classifier, X, y, n_jobs=-1, scoring={'f1 weighted':make_scorer(f1_score, average='weighted'),\n",
        "                                                        'balanced accuracy':make_scorer(balanced_accuracy_score)}, cv=10,\n",
        "                            error_score='raise')\n",
        "    avg_fmeasure = sum(scores['test_f1 weighted'])/len(scores['test_f1 weighted']) # The average f-measure\n",
        "    avg_accuracy = sum(scores['test_balanced accuracy'])/len(scores['test_balanced accuracy']) # The average accuracy\n",
        "    f_measures[name] = avg_fmeasure\n",
        "    accuracies[name] = avg_accuracy\n",
        "# Example f_measures = {'Simple Decision': 0.8551, 'Ensemble with random ...': 0.92, ...}\n",
        "\n",
        "#END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUqhDUuCXMmW",
        "outputId": "f6c17d26-7910-4640-a145-f3ad17cd7cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BaggingClassifier(base_estimator=VotingClassifier(estimators=[('Criterion '\n",
            "                                                               'Entropy',\n",
            "                                                               DecisionTreeClassifier(max_features='auto',\n",
            "                                                                                      random_state=42)),\n",
            "                                                              ('Criterion Log '\n",
            "                                                               'Loss',\n",
            "                                                               DecisionTreeClassifier(max_features='sqrt',\n",
            "                                                                                      random_state=42)),\n",
            "                                                              ('Criterion Gini',\n",
            "                                                               DecisionTreeClassifier(max_features='log2',\n",
            "                                                                                      random_state=42))],\n",
            "                                                  n_jobs=-1),\n",
            "                  n_jobs=-1, random_state=42)\n",
            "BaggingClassifier(base_estimator=VotingClassifier(estimators=[('Criterion '\n",
            "                                                               'Entropy',\n",
            "                                                               DecisionTreeClassifier(max_features='auto',\n",
            "                                                                                      random_state=42)),\n",
            "                                                              ('Criterion Log '\n",
            "                                                               'Loss',\n",
            "                                                               DecisionTreeClassifier(max_features='sqrt',\n",
            "                                                                                      random_state=42)),\n",
            "                                                              ('Criterion Gini',\n",
            "                                                               DecisionTreeClassifier(max_features='log2',\n",
            "                                                                                      random_state=42))],\n",
            "                                                  n_jobs=-1),\n",
            "                  n_jobs=-1, random_state=42)\n",
            "BaggingClassifier(base_estimator=StackingClassifier(estimators=[('Criterion '\n",
            "                                                                 'Entropy',\n",
            "                                                                 DecisionTreeClassifier(max_features='auto',\n",
            "                                                                                        random_state=42)),\n",
            "                                                                ('Criterion '\n",
            "                                                                 'Log Loss',\n",
            "                                                                 DecisionTreeClassifier(max_features='sqrt',\n",
            "                                                                                        random_state=42)),\n",
            "                                                                ('Criterion '\n",
            "                                                                 'Gini',\n",
            "                                                                 DecisionTreeClassifier(max_features='log2',\n",
            "                                                                                        random_state=42))],\n",
            "                                                    n_jobs=-1),\n",
            "                  n_jobs=-1, random_state=42)\n",
            "DecisionTreeClassifier(random_state=42)\n",
            "Classifier:Bugging and Soft Voting Trees -  F1 Weighted:0.7873\n",
            "Classifier:Bugging with Hard Voting Trees -  F1 Weighted:0.7873\n",
            "Classifier:Bugging Stacking Trees -  F1 Weighted:0.7719\n",
            "Classifier:Simple Decision -  F1 Weighted:0.6855\n",
            "Classifier:Bugging and Soft Voting Trees -  BalancedAccuracy:0.777\n",
            "Classifier:Bugging with Hard Voting Trees -  BalancedAccuracy:0.777\n",
            "Classifier:Bugging Stacking Trees -  BalancedAccuracy:0.7515\n",
            "Classifier:Simple Decision -  BalancedAccuracy:0.6754\n"
          ]
        }
      ],
      "source": [
        "print(ens1)\n",
        "print(ens2)\n",
        "print(ens3)\n",
        "print(tree)\n",
        "for name,score in f_measures.items():\n",
        "    print(\"Classifier:{} -  F1 Weighted:{}\".format(name,round(score,4)))\n",
        "for name,score in accuracies.items():\n",
        "    print(\"Classifier:{} -  BalancedAccuracy:{}\".format(name,round(score,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqdXTE_2XMmX"
      },
      "source": [
        "**2.2** Describe your classifiers and your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9POFftXMmX"
      },
      "source": [
        "Έχουμε τρία απλά Decision Trees στα οποία χρησιμοποιούμε τρεις διαφορετικούς τρόπους για την επιλογή χαρακτηριστικών και βάσει αυτών δημιουργούμε τρία διαφορετικά ensembles, ένα voting classifier με soft voting, έναν voting classifier με hard voting και ένα stacking classifier. Παρατηρούμε πως οι voting classifiers, soft και hard, για μια τόσο μικρή τροποποίηση στις υπερπαραμέτρους καταφέρνουν και πετυχαίνουν το ίδιο σκορ, είτε μιλάμε για f1 weighted, είτε για balanced accuracy. Έπειτα, έχουμε τον stacking classifier που πετυχαίνει λίγο χαμηλότερο σκορ. Ωστόσο, αυτό που μας ενδιαφέρει να δώσουμε έμφαση είναι πως όλοι ξεπέρασαν το σκορ του απλά δέντρου αποφάσεων παρά τις αμελητέες αλλαγές στις υπερπαραμέτρους, έτσι αντιλαμβανόμαστε και εμείς οι ίδιοι το πλεονέκτημα των ensemble έναντι των απλών classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkJeuV1FXMmX"
      },
      "source": [
        "**2.3** Increasing the number of estimators in a bagging classifier can drastically increase the training time of a classifier. Is there any solution to this problem? Can the same solution be applied to boosting classifiers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApNEPcWEXMmY"
      },
      "source": [
        "Στους bagging classifiers μπορούμε να εκπαίδευσουμε και να χρησιμοποιήσουμε για προβλέψεις κάθε μοντέλο ξεχωριστά, οπότε μπορούμε να εκπαιδεύουμε περισσότερα του ενός μοντέλου παράλληλα. Έτσι, μπορεί να γίνει χρήση πολλαπλών υπολογιστικών ταυτόχρονα προκειμένου να επιταχύνουμε την εκπαίδευση ενός bagging classifier.\n",
        "Από την άλλη πλευρά δεν ισχύει το ίδιο και για τους boosting classifiers, καθώς αυτοί χαρακτηρίζονται από διαδοχικότητα, αντί για παραλληλία, οπότε είναι και φυσικό να χρειάζεται περισσότερος χρόνος για την εκπαίδευση τέτοιων μοντέλων."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgvsCbUGXMmY"
      },
      "source": [
        "## 3.0 Creating the best classifier ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6daX2mRXMmZ"
      },
      "source": [
        "**3.1** In this part of the assignment you are asked to train the best possible ensemble! Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure (weighted) & balanced accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code. Can you achieve a balanced accuracy over 83-84%?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00xAQ0HfXMmZ"
      },
      "outputs": [],
      "source": [
        "# BEGIN CODE HERE\n",
        "\n",
        "best_cls = StackingClassifier(estimators=[('MLP', MLPClassifier(random_state=RANDOM_STATE)),\n",
        "                                      ('Gradient Boosting',GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
        "                                      ('Linear SVM',LinearSVC(random_state=RANDOM_STATE, max_iter=500000))],\n",
        "                          n_jobs=-1) # Stacking Classifier\n",
        "\n",
        "\n",
        "scores = cross_validate(best_cls, X, y, n_jobs=-1, scoring={'f1 weighted':make_scorer(f1_score, average='weighted'),\n",
        "                                                        'balanced accuracy':make_scorer(balanced_accuracy_score)},\n",
        "                        cv=10, error_score=\"raise\")\n",
        "\n",
        "best_fmeasure = sum(scores['test_f1 weighted'])/len(scores['test_f1 weighted']) # The average f-measure\n",
        "best_accuracy = sum(scores['test_balanced accuracy'])/len(scores['test_balanced accuracy']) # The average accuracy\n",
        "\n",
        "#END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbLB09agXMma",
        "outputId": "be9abfc9-aa0d-4e22-9f07-46926fd1d987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier:\n",
            "StackingClassifier(estimators=[('MLP', MLPClassifier(random_state=42)),\n",
            "                               ('Gradient Boosting',\n",
            "                                GradientBoostingClassifier(random_state=42)),\n",
            "                               ('Linear SVM',\n",
            "                                LinearSVC(max_iter=500000, random_state=42))],\n",
            "                   n_jobs=-1)\n",
            "F1 Weighted-Score:0.8555955691096606 & Balanced Accuracy:0.8489449221507813\n"
          ]
        }
      ],
      "source": [
        "print(\"Classifier:\")\n",
        "print(best_cls)\n",
        "print(\"F1 Weighted-Score:{} & Balanced Accuracy:{}\".format(best_fmeasure, best_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnos1uqzXMma"
      },
      "source": [
        "**3.2** Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5dAfbTfXMmb"
      },
      "source": [
        "Έγιναν πολλές δοκιμές, με συνδυασμούς stacking και voting classifiers. Είτε κάνοντας stacking από voting και άλλους classifier, είτε κάνοντας stacking από voting και άλλους classifiers. Ωστόσο, δεν κατάφερε κανένας να ξεπεράσει τα αποτελέσματα του classifier της ασκέήσεως 1.2. Η λογική που δημιουργήσαμε εκέινον τον classifier ήταν η εξής: Βρίσκουμε πρώτα τους δύο διαφορετικούς classifier που ταιριάζουν καλύτερα μεταξύ τους (MLP & SVM) και ύστερα βρίσκουμε με ποιον άλλον classifier συνεργάζονται εξίσου καλά αυτοί η δύο (Gradient Boosting) και τους συνδυάζουμε. Έν τέλει για να κάνουμε το μοντέλο μας ακόμα λίγο πιο πολύπλοκο δοκιμάσαμε διαφορετικούς classifiers ως τελικούς estimators, παρόλα αυτά αυτό δε μας έφερε τα επιθυμητά αποτελέσματα, καθώς μπορεί να αύξαναν τη μία μετρική εις βάρος της άλλης, οπότε καταλήξαμε να χρησιμοποιούμε και πάλι Linear Regression που φαινόταν κιόλας να απέδιδε αρκέτα καλά στο πρόβλημα ακόμη και μόνη της. Επομένως, καταλήξαμε στον ίδιο Classifier με της άσκησης 1.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQEFCmbcXMmb"
      },
      "source": [
        "**3.3** Create a classifier that is going to be used in production - in a live system. Use the *test_set_noclass.csv* to make predictions. Store the predictions in a list.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQPgm_ubXMmc",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# BEGIN CODE HERE\n",
        "data = pd.read_csv('train_set.csv')\n",
        "X = data.drop(columns=['CLASS']).values\n",
        "y = data['CLASS'].values\n",
        "sss = StratifiedShuffleSplit(n_splits = 10, test_size = 0.33, random_state=RANDOM_STATE)\n",
        "\n",
        "best = [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]\n",
        "best_train = [[[], []], [[], []], [[], []]]\n",
        "for train_indexes, test_indexes in sss.split(X, y):\n",
        "    X_train, X_test = X[train_indexes], X[test_indexes]\n",
        "    y_train, y_test = y[train_indexes], y[test_indexes]\n",
        "    classifier = StackingClassifier(estimators=[('MLP', MLPClassifier(random_state=RANDOM_STATE)),\n",
        "                                      ('Gradient Boosting',GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
        "                                      ('Linear SVM',LinearSVC(random_state=RANDOM_STATE, max_iter=500000))],\n",
        "                              n_jobs=-1) # Stacking Classifier 0\n",
        "    classifier.fit(X_train,y_train)\n",
        "    y_predict = classifier.predict(X_test)\n",
        "    fmeasure = f1_score(y_test, y_predict, average='weighted')\n",
        "    accuracy = balanced_accuracy_score(y_test, y_predict)\n",
        "\n",
        "    if fmeasure > best[0][0]:\n",
        "        best[0][0] = fmeasure\n",
        "        best[0][1] = accuracy\n",
        "        best_train[0][0] = X_train\n",
        "        best_train[0][1] = y_train\n",
        "\n",
        "    if accuracy > best[1][1]:\n",
        "        best[1][0] = fmeasure\n",
        "        best[1][1] = accuracy\n",
        "        best_train[1][0] = X_train\n",
        "        best_train[1][1] = y_train\n",
        "\n",
        "    if fmeasure > best[2][0] and accuracy > best[2][1]:\n",
        "        best[2][0] = fmeasure\n",
        "        best[2][1] = accuracy\n",
        "        best_train[2][0] = X_train\n",
        "        best_train[2][1] = y_train\n",
        "\n",
        "# best = [[0.85425292-15319144, 0.84-1473921780401-1],\n",
        "        # [0.85382512-10873148, 0.8474281079359889],\n",
        "        # [0.85382512-10873148, 0.8474281079359889]]\n",
        "\n",
        "cls = classifier = StackingClassifier(estimators=[('MLP', MLPClassifier(random_state=RANDOM_STATE)),\n",
        "                                      ('Gradient Boosting',GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
        "                                      ('Linear SVM',LinearSVC(random_state=RANDOM_STATE, max_iter=500000))],\n",
        "                              n_jobs=-1) # Stacking Classifier 0\n",
        "\n",
        "cls.fit(best_train[2][0], best_train[2][1])\n",
        "\n",
        "# END CODE HERE\n",
        "test_set = pd.read_csv(\"test_set_noclass.csv\")\n",
        "predictions = cls.predict(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnAp-d2DXMmf"
      },
      "source": [
        "Το μόνο σχόλιο που χρειάζεται είναι πως το εκπαιδεύουμε με ένα μέρος του dataset (70%) το οποίο είναι Stratified και χρησιμοποιούμε το υπόλοιπο (30%) για να ελέγξουμε την ποιότητα του split. Έπειτα διαλέγουμε το καλύτερο split βάσει μίας από τις δύο μετρικές ή και των δύο ταυτόχρονα (έγινε σύμφωνα με τη προσωπική μας κρίση αυτή τη φορά η επιλογή και κατά πόσο απόκλιση υπήρχε ανάμεσα στα μέγιστα σκορ τους) και το εκπαιδεύουμε με αυτό."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Neagvu0TXMmg"
      },
      "source": [
        "#### This following cell will not be executed. The test_set.csv with the classes will be made available after the deadline and this cell is for testing purposes!!! Do not modify it! ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7K7iI7BXMmg"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from sklearn.metrics import f1_score, balanced_accuracy_score\n",
        "    final_test_set = pd.read_csv('test_set.csv')\n",
        "    ground_truth = final_test_set['CLASS']\n",
        "    print(\"Balanced Accuracy: {}\".format(balanced_accuracy_score(predictions, ground_truth)))\n",
        "    print(\"F1 Weighted-Score: {}\".format(f1_score(predictions, ground_truth, average='weighted')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJH-9KdOzW7z"
      },
      "source": [
        "Both should aim above 85%!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}